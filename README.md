# codechat-eval

Analyzing the quality relationship between prompts and generated code.

Based on the [CodeChat-V2.0](https://huggingface.co/datasets/Suzhen/CodeChat-V2.0) dataset.

### License

This project is licensed under the [MIT License][license].

### Citation

```
@misc{zhong2025developerllmconversationsempiricalstudy,
      title={Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality},
      author={Suzhen Zhong and Ying Zou and Bram Adams},
      year={2025},
      eprint={2509.10402},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2509.10402},
}
```

### Extra Info

CodeChat-V2.0 is a large-scale dataset comprising 587,568 real-world developerâ€“LLM conversations, derived from the [WildChat][wildchat] dataset.

- Paper: <https://arxiv.org/abs/2509.10402>
- GitHub: <https://github.com/Software-Evolution-Analytics-Lab-SEAL/CodeChat>

[license]: https://github.com/darragh0/codechat-eval?tab=MIT-1-ov-file# "codechat-eval license"
[wildchat]: https://huggingface.co/datasets/allenai/WildChat "wildchat dataset"
